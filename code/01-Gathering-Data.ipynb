{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddit Spam Classifier\n",
    "#### Web APIs & Classification\n",
    "_Author: Ritchie Kwan_\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "0. [Problem Statement, Assumptions, Executive Summary](#Problem-Statement)\n",
    "1. [Data Collection](#Data-Collection)\n",
    "1. [Data Cleaning & EDA](02-Data-Cleaning-and-EDA.ipynb#Data-Cleaning-and-EDA)\n",
    "1. [Benchmark Model](03-Benchmark-Model.ipynb#Benchmark-Model)\n",
    "1. [Model Tuning](04-Model-Tuning.ipynb#Model-Tuning)\n",
    "1. [Evaluation and Conceptual Understanding](04-Model-Tuning.ipynb#Evaluation-and-Conceptual-Understanding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Spam and ham are typically used to describe junk and relevant emails, respectively. In the context of Reddit, any post unrelated to the subreddit can be considered \"spam\", while posts related to the subreddit are \"ham\". The integrity of a subreddit relies on the quality and relevance of its posts. The content of a spam post could be as bad as a hyperlink to a phishing website or as benign as a high-quality post accidentally submitted to the wrong subreddit. \n",
    "\n",
    "A spam detection bot could be trained to automatically detect and remove irrelevant posts before they are released to the public. **The objective is to determine how the frequency of certain two-word phrases (bi-grams) in a post affect the odds that the post belongs to a specific subreddit by using Neuro-Linguistic Programming (NLP) techniques.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "The default page of each subreddit is sorted by `hot`, typically posts that have already acquired a sufficient amount of upvotes by users and possibly passed a spam detection bot. Since the posts collected using the Reddit API appear to be sorted the same way, I assume that **all posts collected using the Reddit API are ham**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "**Data Collection**  \n",
    "I collected as many posts from `/r/leanfire` and `/r/fatfire` as Reddit would allow. In addition to `title` and `selftext`, I also collected `comments` to train my models. Since I collected more posts from `/r/leanfire`, it will be my target class `y=1` (ham). Posts from `/r/fatfire` will be used to imitate spam for the sake of training classification models. \n",
    "\n",
    "**Data Cleaning and EDA**  \n",
    "I extracted text from the `title`, `selftext`, and `comments` columns to use as my predictors. I used `RegExpTokenizer` to split the text into individual words and `Lemmatizer` to reduce words to their lemma. \n",
    "\n",
    "**Benchmark Model**  \n",
    "I train test split my data to check for overfitting. I used `CountVectorizer` and `TfidfVectorizer` to map bi-grams to their frequency or weighted frequency in each post. For my benchmark model, I used `LogisticRegression`. For the training data, I used **cross validated accuracy score to measure success** of the model. Not only did the model perform better using the `TfidfVectorizer` transformed data, the highest ranked bi-grams were also more meaningful. \n",
    "\n",
    "**Model Tuning**  \n",
    "I used the following classification models with and without tuning hyperparameters:\n",
    "- LogisticRegression\n",
    "- KNN\n",
    "- NaiveBayes\n",
    "    - MultinomialNB\n",
    "- DecisionTreeClassifier\n",
    "- BaggingClassifier\n",
    "- RandomForestClassifier\n",
    "- ExtraTreesClassifier\n",
    "- AdaBoostClassifier\n",
    "- SVC\n",
    "- VotingClassifer\n",
    "\n",
    "With and without hyperparametering tuning, Naive Bayes' `MultinomialNB` performed the best.\n",
    "\n",
    "**Evaluation and Conceptual Understanding**  \n",
    "Unseen data will be newly submitted posts on `/r/leanfire`, meaning all new data belongs to the positive class (`y=1`). If the model classifies a post as negative (`y=0`), then it is likely to be spam.  \n",
    "\n",
    "True Positives = Ham  \n",
    "False Negative = Spam  \n",
    "\n",
    "To maintain the integrity of the subreddit, I prefer the model to have a **sufficiently high False Negative rate** (spam detection rate). The consequence of this decision is that **some quality posts may be blocked**. This is preferred over **letting actual spam bypass the detection algorithm**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = ['fatfire', 'leanfire']\n",
    "# subs = ['lifeprotips', 'shittylifeprotips']\n",
    "\n",
    "possible_subs = [('animalsbeingbros', 'animalsbeingjerks'), \n",
    "                 ('powerlifting', 'weightlifting'),\n",
    "                 ('warriors', 'lakers'),\n",
    "                 ('funny', 'cute'),\n",
    "                 ('lifeprotips', 'shittylifeprotips'),\n",
    "                 ('news', 'politics'),\n",
    "                 ('aww', 'lookatmydog') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to get all comments in a reddit post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(children):\n",
    "    '''\n",
    "    Arguments: \n",
    "    children : list of child comment JSONs.\n",
    "    \n",
    "    Recursively extracts all of the comments of a Reddit post.\n",
    "    \n",
    "    Return:\n",
    "    comments : List of all comments in a reddit post.\n",
    "    '''\n",
    "    comments = []\n",
    "    \n",
    "    for child in children:\n",
    "        if child['kind'] == 'more':\n",
    "            continue\n",
    "        \n",
    "        data = child['data']\n",
    "\n",
    "        comments.append(data['body'])        \n",
    "        # if child has children, get comments of children\n",
    "        if data['replies'] != '':\n",
    "            # get_comments returns a list, so .extend() it\n",
    "            comments.extend(get_comments(data['replies']['data']['children']))\n",
    "    \n",
    "    return comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hit the Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keys to note:  \n",
    "`permalink` : to get comments  \n",
    "`title` : predictor words  \n",
    "`selftext` : more predictor words  \n",
    "`body` : body of comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to collect the posts and comments of a subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape(subreddit):\n",
    "    '''\n",
    "    Arguments:\n",
    "    subreddit : String of a subreddit\n",
    "        example: 'leanfire'\n",
    "    \n",
    "    Requests as many posts from Reddit as possible\n",
    "    Stores the text of every post's title, selftext and comments\n",
    "\n",
    "    Return:\n",
    "    A dataframe of post titles, selftext and comments\n",
    "    '''\n",
    "    headers = {'User-agent' : 'Reechee'}\n",
    "    subreddit_url = 'https://www.reddit.com/r/' + subreddit + '.json'\n",
    "    posts = []\n",
    "    after = ''\n",
    "    \n",
    "    # Keep requesting JSONs until Reddit gives up\n",
    "    while after != None:        \n",
    "        # Request subreddit JSON for next 100 posts\n",
    "        req = requests.get(subreddit_url,\n",
    "                                      headers = headers,\n",
    "                                      params = {'after' : after,\n",
    "                                                'limit' : '100'})\n",
    "        subreddit_json = req.json()\n",
    "        \n",
    "        # dictionary of post data\n",
    "        post_data = subreddit_json['data']\n",
    "        \n",
    "        n_posts = len(post_data['children'])\n",
    "        \n",
    "        # For each post webpage, request the JSON\n",
    "        for i, post in enumerate(post_data['children']):\n",
    "            post_url = 'https://www.reddit.com' + post['data']['permalink'] + '.json'\n",
    "            \n",
    "            # Request the post webpage's JSON\n",
    "            post_req = requests.get(post_url, \n",
    "                                     headers = headers)\n",
    "            post_json = post_req.json()\n",
    "\n",
    "            # Get all comments of a post\n",
    "            post['data']['comments'] = get_comments(post_json[1]['data']['children'])\n",
    "            \n",
    "            # Print progress\n",
    "#             print('[{}/{}]{} comments from {}'\n",
    "#                   .format(i+1, n_posts,\n",
    "#                   len(post['data']['comments']),\n",
    "#                   post['data']['permalink']))\n",
    "        \n",
    "        # extend list of posts\n",
    "        posts.extend([post['data'] \\\n",
    "                         for post in post_data['children']])\n",
    "        \n",
    "        # 'after' key for next batch of posts\n",
    "        after = post_data['after']\n",
    "        \n",
    "        print(f'Fetching next 100 posts with after key: {after}')\n",
    "    \n",
    "    print('Done!')\n",
    "    return pd.DataFrame(posts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df0 = scrape(subs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = scrape(subs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['approved_at_utc', 'approved_by', 'archived', 'author',\n",
       "       'author_cakeday', 'author_flair_background_color',\n",
       "       'author_flair_css_class', 'author_flair_richtext',\n",
       "       'author_flair_template_id', 'author_flair_text',\n",
       "       'author_flair_text_color', 'author_flair_type', 'author_fullname',\n",
       "       'author_patreon_flair', 'banned_at_utc', 'banned_by', 'can_gild',\n",
       "       'can_mod_post', 'category', 'clicked', 'comments', 'content_categories',\n",
       "       'contest_mode', 'created', 'created_utc', 'crosspost_parent',\n",
       "       'crosspost_parent_list', 'distinguished', 'domain', 'downs', 'edited',\n",
       "       'gilded', 'gildings', 'hidden', 'hide_score', 'id', 'is_crosspostable',\n",
       "       'is_meta', 'is_original_content', 'is_reddit_media_domain',\n",
       "       'is_robot_indexable', 'is_self', 'is_video', 'likes',\n",
       "       'link_flair_background_color', 'link_flair_css_class',\n",
       "       'link_flair_richtext', 'link_flair_template_id', 'link_flair_text',\n",
       "       'link_flair_text_color', 'link_flair_type', 'locked', 'media',\n",
       "       'media_embed', 'media_only', 'mod_note', 'mod_reason_by',\n",
       "       'mod_reason_title', 'mod_reports', 'name', 'no_follow', 'num_comments',\n",
       "       'num_crossposts', 'num_reports', 'over_18', 'parent_whitelist_status',\n",
       "       'permalink', 'pinned', 'post_hint', 'preview', 'pwls', 'quarantine',\n",
       "       'removal_reason', 'report_reasons', 'saved', 'score', 'secure_media',\n",
       "       'secure_media_embed', 'selftext', 'selftext_html', 'send_replies',\n",
       "       'spoiler', 'stickied', 'subreddit', 'subreddit_id',\n",
       "       'subreddit_name_prefixed', 'subreddit_subscribers', 'subreddit_type',\n",
       "       'suggested_sort', 'thumbnail', 'thumbnail_height', 'thumbnail_width',\n",
       "       'title', 'ups', 'url', 'user_reports', 'view_count', 'visited',\n",
       "       'whitelist_status', 'wls'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>...</th>\n",
       "      <th>thumbnail_height</th>\n",
       "      <th>thumbnail_width</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>url</th>\n",
       "      <th>user_reports</th>\n",
       "      <th>view_count</th>\n",
       "      <th>visited</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>wls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>leedar1376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Advisor fees at various asset levels?</td>\n",
       "      <td>20</td>\n",
       "      <td>https://www.reddit.com/r/fatFIRE/comments/a815...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>a_random_tomato</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HSA record keeping</td>\n",
       "      <td>39</td>\n",
       "      <td>https://www.reddit.com/r/fatFIRE/comments/a7s7...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>retiringearly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019 Money Goals</td>\n",
       "      <td>79</td>\n",
       "      <td>https://www.reddit.com/r/fatFIRE/comments/a7f3...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>kernelcrop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What’s is your Tax Loss Harvesting Threshold?</td>\n",
       "      <td>15</td>\n",
       "      <td>https://www.reddit.com/r/fatFIRE/comments/a7fu...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>TreesButterPanny</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Might be retiring earlier than planned, hoping...</td>\n",
       "      <td>40</td>\n",
       "      <td>https://www.reddit.com/r/fatFIRE/comments/a76b...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  approved_at_utc approved_by  archived            author author_cakeday  \\\n",
       "0            None        None     False        leedar1376            NaN   \n",
       "1            None        None     False   a_random_tomato            NaN   \n",
       "2            None        None     False     retiringearly            NaN   \n",
       "3            None        None     False        kernelcrop            NaN   \n",
       "4            None        None     False  TreesButterPanny            NaN   \n",
       "\n",
       "  author_flair_background_color author_flair_css_class author_flair_richtext  \\\n",
       "0                          None                   None                    []   \n",
       "1                          None                   None                    []   \n",
       "2                          None                   None                    []   \n",
       "3                          None                   None                    []   \n",
       "4                          None                   None                    []   \n",
       "\n",
       "  author_flair_template_id author_flair_text  ...  thumbnail_height  \\\n",
       "0                     None              None  ...               NaN   \n",
       "1                     None              None  ...               NaN   \n",
       "2                     None              None  ...               NaN   \n",
       "3                     None              None  ...               NaN   \n",
       "4                     None              None  ...               NaN   \n",
       "\n",
       "  thumbnail_width                                              title ups  \\\n",
       "0             NaN              Advisor fees at various asset levels?  20   \n",
       "1             NaN                                 HSA record keeping  39   \n",
       "2             NaN                                   2019 Money Goals  79   \n",
       "3             NaN      What’s is your Tax Loss Harvesting Threshold?  15   \n",
       "4             NaN  Might be retiring earlier than planned, hoping...  40   \n",
       "\n",
       "                                                 url user_reports  view_count  \\\n",
       "0  https://www.reddit.com/r/fatFIRE/comments/a815...           []        None   \n",
       "1  https://www.reddit.com/r/fatFIRE/comments/a7s7...           []        None   \n",
       "2  https://www.reddit.com/r/fatFIRE/comments/a7f3...           []        None   \n",
       "3  https://www.reddit.com/r/fatFIRE/comments/a7fu...           []        None   \n",
       "4  https://www.reddit.com/r/fatFIRE/comments/a76b...           []        None   \n",
       "\n",
       "   visited whitelist_status   wls  \n",
       "0    False             None  None  \n",
       "1    False             None  None  \n",
       "2    False             None  None  \n",
       "3    False             None  None  \n",
       "4    False             None  None  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save subreddit posts separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.to_csv(f'../data/{subs[0]}.csv', index = False)\n",
    "df1.to_csv(f'../data/{subs[1]}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the data, shuffle and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/ipykernel/__main__.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df0, df1], axis = 0)\n",
    "df = df.sample(frac = 1, random_state = 42).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "leanfire    999\n",
       "fatFIRE     764\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are sufficiently balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "leanfire    0.566648\n",
       "fatFIRE     0.433352\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'../data/{subs[1]}-{subs[0]}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
